{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11831593,"sourceType":"datasetVersion","datasetId":7432907}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:54:09.984293Z","iopub.execute_input":"2025-05-16T14:54:09.984581Z","iopub.status.idle":"2025-05-16T14:54:09.989014Z","shell.execute_reply.started":"2025-05-16T14:54:09.984560Z","shell.execute_reply":"2025-05-16T14:54:09.988184Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"BASE_DIR = \"/kaggle/input/imgur5k/cropped/\"\nTRAIN_DIR = os.path.join(BASE_DIR, \"train\")\nVAL_DIR = os.path.join(BASE_DIR, \"val\")\nTEST_DIR = os.path.join(BASE_DIR, \"test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:59:06.548008Z","iopub.execute_input":"2025-05-16T14:59:06.548709Z","iopub.status.idle":"2025-05-16T14:59:06.552421Z","shell.execute_reply.started":"2025-05-16T14:59:06.548684Z","shell.execute_reply":"2025-05-16T14:59:06.551677Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def load_json_to_df(json_path):\n    df = pd.read_json(json_path)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:54:38.779825Z","iopub.execute_input":"2025-05-16T14:54:38.780078Z","iopub.status.idle":"2025-05-16T14:54:38.785757Z","shell.execute_reply.started":"2025-05-16T14:54:38.780049Z","shell.execute_reply":"2025-05-16T14:54:38.784333Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"processor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T15:34:29.010716Z","iopub.execute_input":"2025-05-16T15:34:29.011371Z","iopub.status.idle":"2025-05-16T15:34:32.839370Z","shell.execute_reply.started":"2025-05-16T15:34:29.011343Z","shell.execute_reply":"2025-05-16T15:34:32.838687Z"}},"outputs":[{"name":"stderr","text":"Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 1024,\n  \"image_size\": 384,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 16,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 24,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 1024,\n  \"qkv_bias\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\"\n}\n\nConfig of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_cross_attention\": true,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": 0.0,\n  \"cross_attention_hidden_size\": 1024,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"eos_token_id\": 2,\n  \"init_std\": 0.02,\n  \"is_decoder\": true,\n  \"layernorm_embedding\": true,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"trocr\",\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": false,\n  \"use_learned_position_embeddings\": true,\n  \"vocab_size\": 50265\n}\n\nSome weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"class Imgur5KDataset(Dataset):\n    def __init__(self, image_dir, json_file, processor, transform):\n        self.image_dir = image_dir\n        self.df = load_json_to_df(image_dir+json_file)\n        self.df[\"image_name\"] = self.df[\"image_name\"] + \".png\"\n        self.processor = processor\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image_path = os.path.join(self.image_dir, self.df.iloc[idx][\"image_name\"])\n        text = self.df.iloc[idx][\"text\"]\n\n        # Open image\n        image = Image.open(image_path).convert(\"RGB\")\n        image = self.transform(image)\n\n        # Process image with TrOCR processor\n        pixel_values = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n\n        # Process text with TrOCR tokenizer\n        labels = self.processor.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True).input_ids.squeeze()\n\n        return {\n            \"pixel_values\": pixel_values,\n            \"labels\": labels\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T15:34:32.840702Z","iopub.execute_input":"2025-05-16T15:34:32.841043Z","iopub.status.idle":"2025-05-16T15:34:32.847888Z","shell.execute_reply.started":"2025-05-16T15:34:32.841018Z","shell.execute_reply":"2025-05-16T15:34:32.847120Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"# Define the image transformation pipeline\nimage_transform = transforms.Compose([\n    transforms.ToTensor(),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T15:34:34.237623Z","iopub.execute_input":"2025-05-16T15:34:34.238369Z","iopub.status.idle":"2025-05-16T15:34:34.241773Z","shell.execute_reply.started":"2025-05-16T15:34:34.238344Z","shell.execute_reply":"2025-05-16T15:34:34.241126Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"train_dataset = Imgur5KDataset(\n    TRAIN_DIR, \n    \"/words.json\", \n    processor, \n    image_transform\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T15:34:35.966559Z","iopub.execute_input":"2025-05-16T15:34:35.967317Z","iopub.status.idle":"2025-05-16T15:34:36.157342Z","shell.execute_reply.started":"2025-05-16T15:34:35.967294Z","shell.execute_reply":"2025-05-16T15:34:36.156730Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"val_dataset = Imgur5KDataset(\n    VAL_DIR, \n    \"/words.json\", \n    processor, \n    image_transform\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T15:34:37.824826Z","iopub.execute_input":"2025-05-16T15:34:37.825472Z","iopub.status.idle":"2025-05-16T15:34:37.865336Z","shell.execute_reply.started":"2025-05-16T15:34:37.825448Z","shell.execute_reply":"2025-05-16T15:34:37.864719Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"test_dataset = Imgur5KDataset(\n    TEST_DIR, \n    \"/words.json\", \n    processor, \n    image_transform\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T15:34:39.208721Z","iopub.execute_input":"2025-05-16T15:34:39.209345Z","iopub.status.idle":"2025-05-16T15:34:39.251229Z","shell.execute_reply.started":"2025-05-16T15:34:39.209321Z","shell.execute_reply":"2025-05-16T15:34:39.250443Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=3,shuffle=True,num_workers=4,pin_memory= True)\nval_loader = DataLoader(val_dataset, batch_size=3,num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=3,num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T15:50:37.048416Z","iopub.execute_input":"2025-05-16T15:50:37.048744Z","iopub.status.idle":"2025-05-16T15:50:37.053965Z","shell.execute_reply.started":"2025-05-16T15:50:37.048715Z","shell.execute_reply":"2025-05-16T15:50:37.053231Z"}},"outputs":[],"execution_count":101},{"cell_type":"code","source":"# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T15:34:42.910198Z","iopub.execute_input":"2025-05-16T15:34:42.910755Z","iopub.status.idle":"2025-05-16T15:34:43.844009Z","shell.execute_reply.started":"2025-05-16T15:34:42.910731Z","shell.execute_reply":"2025-05-16T15:34:43.843344Z"}},"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"VisionEncoderDecoderModel(\n  (encoder): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=False)\n              (key): Linear(in_features=1024, out_features=1024, bias=False)\n              (value): Linear(in_features=1024, out_features=1024, bias=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViTPooler(\n      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): TrOCRForCausalLM(\n    (model): TrOCRDecoderWrapper(\n      (decoder): TrOCRDecoder(\n        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (layers): ModuleList(\n          (0-11): 12 x TrOCRDecoderLayer(\n            (self_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (activation_fn): GELUActivation()\n            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n  )\n)"},"metadata":{}}],"execution_count":89},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom torch.cuda.amp import GradScaler, autocast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T15:34:47.812444Z","iopub.execute_input":"2025-05-16T15:34:47.812716Z","iopub.status.idle":"2025-05-16T15:34:47.816615Z","shell.execute_reply.started":"2025-05-16T15:34:47.812696Z","shell.execute_reply":"2025-05-16T15:34:47.816031Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"# Define optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Set up gradient scaler for mixed-precision training\nscaler = torch.amp.GradScaler()\n\n# Number of epochs for fine-tuning\nepochs = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T15:34:49.079144Z","iopub.execute_input":"2025-05-16T15:34:49.079621Z","iopub.status.idle":"2025-05-16T15:34:49.085358Z","shell.execute_reply.started":"2025-05-16T15:34:49.079598Z","shell.execute_reply":"2025-05-16T15:34:49.084466Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"from tqdm import tqdm\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Initialize scaler\nscaler = torch.cuda.amp.GradScaler('cuda')\n\n# Set the decoder_start_token_id and pad_token_id before training\nif model.config.decoder_start_token_id is None:\n    model.config.decoder_start_token_id = processor.tokenizer.cls_token_id  # or processor.tokenizer.bos_token_id\n\n# Set the pad_token_id if it's not set\nif model.config.pad_token_id is None:\n    model.config.pad_token_id = processor.tokenizer.pad_token_id  # Typically, the [PAD] token ID\n\n# Training loop\ndef train(model, train_loader, val_loader, optimizer, scaler, epochs=10):\n    for epoch in range(epochs):\n        model.train()  # Set model to training mode\n        epoch_loss = 0\n\n        # Loop over training batches\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            pixel_values = batch[\"pixel_values\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            # Zero the gradients\n            optimizer.zero_grad()\n\n            # Mixed-precision context\n            with autocast():\n                outputs = model(pixel_values=pixel_values, labels=labels)\n                loss = outputs.loss\n\n            # Scaled loss backpropagation\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            epoch_loss += loss.item()\n\n        print(f\"Epoch {epoch+1} Loss: {epoch_loss / len(train_loader)}\")\n        \n        # Validate after each epoch\n        evaluate(model, val_loader)\n    \n    # Save the fine-tuned model after training\n    model.save_pretrained(\"/kaggle/working/trocr_finetuned\")\n    print(\"Fine-tuned model saved.\")\n\n# Call the training function\ntrain(model, train_loader, val_loader, optimizer, scaler, epochs=epochs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T15:50:41.639633Z","iopub.execute_input":"2025-05-16T15:50:41.640448Z","iopub.status.idle":"2025-05-16T15:50:42.271924Z","shell.execute_reply.started":"2025-05-16T15:50:41.640425Z","shell.execute_reply":"2025-05-16T15:50:42.271043Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/1253814446.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler('cuda')\nEpoch 1/10:   0%|          | 0/39774 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nEpoch 1/10:   0%|          | 0/39774 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1253814446.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Call the training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/1253814446.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, scaler, epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Loop over training batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 39.12 MiB is free. Process 5618 has 15.85 GiB memory in use. Of the allocated memory 15.43 GiB is allocated by PyTorch, and 125.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 39.12 MiB is free. Process 5618 has 15.85 GiB memory in use. Of the allocated memory 15.43 GiB is allocated by PyTorch, and 125.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":102},{"cell_type":"code","source":"from datasets import load_metric\nimport numpy as np\n\n# Load evaluation metrics\ncer_metric = load_metric(\"cer\")\nwer_metric = load_metric(\"wer\")\n\n# Evaluation function\ndef evaluate(model, data_loader):\n    model.eval()  # Set model to evaluation mode\n\n    all_predictions = []\n    all_labels = []\n\n    for batch in tqdm(data_loader, desc=\"Evaluating\"):\n        pixel_values = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Inference (disable gradient calculation)\n        with torch.no_grad():\n            generated_ids = model.generate(pixel_values)\n\n        # Decode predictions and labels\n        decoded_preds = processor.batch_decode(generated_ids, skip_special_tokens=True)\n        decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)\n\n        all_predictions.extend(decoded_preds)\n        all_labels.extend(decoded_labels)\n\n    # Calculate CER and WER\n    cer_score = cer_metric.compute(predictions=all_predictions, references=all_labels)\n    wer_score = wer_metric.compute(predictions=all_predictions, references=all_labels)\n\n    print(f\"CER: {cer_score:.4f}, WER: {wer_score:.4f}\")\n    return cer_score, wer_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate on the validation set\nprint(\"Evaluating on Validation Set...\")\nval_cer, val_wer = evaluate(model, val_loader)\n\n# Evaluate on the test set\nprint(\"Evaluating on Test Set...\")\ntest_cer, test_wer = evaluate(model, test_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the fine-tuned model to disk\nmodel.save_pretrained(\"/kaggle/working/trocr_finetuned\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}